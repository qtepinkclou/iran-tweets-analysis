{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "Main goal of this work is to explain the basics of what is happening during the protests in Iran thorugh the tweets dataset. We are looking for the answers of the 3 questions listed below that will help to unravel the mysteries behind the protests:\n",
    "1. What are the relationships between the most common words (hashtags) used in the tweets?\n",
    "2. -\n",
    "3. -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "In this part we load the dataset into a pandas DataFrame and check for inconsistencies, missing values and other possible problems which may get in the way of a proper analysis of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_wrangler import Wrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Heidi 🌊💙🇺🇦🇺🇸🇸🇰🌻🪴☕️🦅</td>\n",
       "      <td>Plainville, CT</td>\n",
       "      <td>I enjoy connecting w/ppl around the 🌍 Democrat...</td>\n",
       "      <td>2010-10-03 04:41:23+00:00</td>\n",
       "      <td>5916.0</td>\n",
       "      <td>6472</td>\n",
       "      <td>177328</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-12-02 16:38:01+00:00</td>\n",
       "      <td>Don’t Let Them Stand Alone. The #women + Girls...</td>\n",
       "      <td>['women', 'IranianRegime']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Captain Merika</td>\n",
       "      <td>earth 🌍🌎</td>\n",
       "      <td>I hate all forms of dictatorship</td>\n",
       "      <td>2012-04-02 20:18:17+00:00</td>\n",
       "      <td>51.0</td>\n",
       "      <td>85</td>\n",
       "      <td>43</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-12-02 16:35:59+00:00</td>\n",
       "      <td>🇮🇷Mina Yagoubi, 33, a citizen of Arak, arreste...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>marjan nourai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in the now</td>\n",
       "      <td>2021-01-06 22:23:55+00:00</td>\n",
       "      <td>72.0</td>\n",
       "      <td>92</td>\n",
       "      <td>14751</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-12-02 16:33:47+00:00</td>\n",
       "      <td>Tweeting isn’t enough! Social Media isn’t enou...</td>\n",
       "      <td>['WomanLifeFreedom', 'IranProtests2022', 'Iran...</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IranWire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>News and stories from the heart of #Iran.</td>\n",
       "      <td>2013-04-17 12:59:02+00:00</td>\n",
       "      <td>37877.0</td>\n",
       "      <td>1291</td>\n",
       "      <td>6455</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-12-02 16:29:00+00:00</td>\n",
       "      <td>At #Iran's temporary detention centres, for th...</td>\n",
       "      <td>['Iran']</td>\n",
       "      <td>Buffer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hamidreza Azizi</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>PhD | Visiting Fellow @SWPBerlin | Associate @...</td>\n",
       "      <td>2012-12-04 12:22:28+00:00</td>\n",
       "      <td>4523.0</td>\n",
       "      <td>470</td>\n",
       "      <td>34219</td>\n",
       "      <td>True</td>\n",
       "      <td>2022-12-02 16:28:44+00:00</td>\n",
       "      <td>There are at least two problems with this ethn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_name   user_location  \\\n",
       "0  Heidi 🌊💙🇺🇦🇺🇸🇸🇰🌻🪴☕️🦅  Plainville, CT   \n",
       "1       Captain Merika        earth 🌍🌎   \n",
       "2        marjan nourai             NaN   \n",
       "3             IranWire             NaN   \n",
       "4      Hamidreza Azizi          Berlin   \n",
       "\n",
       "                                    user_description  \\\n",
       "0  I enjoy connecting w/ppl around the 🌍 Democrat...   \n",
       "1                   I hate all forms of dictatorship   \n",
       "2                                         in the now   \n",
       "3          News and stories from the heart of #Iran.   \n",
       "4  PhD | Visiting Fellow @SWPBerlin | Associate @...   \n",
       "\n",
       "                user_created user_followers user_friends user_favourites  \\\n",
       "0  2010-10-03 04:41:23+00:00         5916.0         6472          177328   \n",
       "1  2012-04-02 20:18:17+00:00           51.0           85              43   \n",
       "2  2021-01-06 22:23:55+00:00           72.0           92           14751   \n",
       "3  2013-04-17 12:59:02+00:00        37877.0         1291            6455   \n",
       "4  2012-12-04 12:22:28+00:00         4523.0          470           34219   \n",
       "\n",
       "  user_verified                       date  \\\n",
       "0         False  2022-12-02 16:38:01+00:00   \n",
       "1         False  2022-12-02 16:35:59+00:00   \n",
       "2         False  2022-12-02 16:33:47+00:00   \n",
       "3         False  2022-12-02 16:29:00+00:00   \n",
       "4          True  2022-12-02 16:28:44+00:00   \n",
       "\n",
       "                                                text  \\\n",
       "0  Don’t Let Them Stand Alone. The #women + Girls...   \n",
       "1  🇮🇷Mina Yagoubi, 33, a citizen of Arak, arreste...   \n",
       "2  Tweeting isn’t enough! Social Media isn’t enou...   \n",
       "3  At #Iran's temporary detention centres, for th...   \n",
       "4  There are at least two problems with this ethn...   \n",
       "\n",
       "                                            hashtags               source  \n",
       "0                         ['women', 'IranianRegime']   Twitter for iPhone  \n",
       "1                                                NaN  Twitter for Android  \n",
       "2  ['WomanLifeFreedom', 'IranProtests2022', 'Iran...   Twitter for iPhone  \n",
       "3                                           ['Iran']               Buffer  \n",
       "4                                                NaN   Twitter for iPhone  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('tweets.csv', dtype=object)\n",
    "wrangle = Wrangler()\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "\n",
    "In this part we will pre-process our data for it to be ready for the actual analysis in terms of finding the answers to our questions listed at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of tweets removed due to having invalid sources is:\n",
      "6.897837084370938\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.dropna(\n",
    "    subset=['user_verified', 'text', 'user_followers', 'user_created', 'user_friends', 'user_favourites', 'date', 'source', 'user_name']\n",
    ")\n",
    "\n",
    "valid_sources = [\n",
    "    'Twitter for Android',\n",
    "    'Twitter for iPhone',\n",
    "    'Twitter Web App',\n",
    "    'Twitter for iPad'\n",
    "]\n",
    "valid_src_tweets = tweets.loc[tweets['source'].isin(valid_sources)]\n",
    "\n",
    "print('Percentage of tweets removed due to having invalid sources is:')\n",
    "print((len(tweets) - len(valid_src_tweets))/len(tweets) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from a few vital missing values in a small subset of tweets, I have realised that a considerable amount of tweets were posted using sources that are non-Twitter applications and third-party clients. Further research on internet showed that tweets with non-Twitter application sources point to user accounts likely to be managed by bots as well as tweets posted through validated Twitter applications are likely to be human beings. Therefore I have decided to filter out any tweets (rows) form the dataset which does not have a valid Twitter application as its source."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further research showed that even state of the art bot detection algorithms for twitter depends heavily on the source of the tweet and therefore validated my approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are further cleaning and preparation of the data that is employed through the Wrangler class such as removal of stop words, various regex impressions to disect the text into different types of words etc. Please refer to the data_wrangler.py for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: What are the relationships between the most common words (hashtags) used in the tweets?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the `Wrangle` class, we will be able to take a look at each text of the tweet, remove links, group hashtags and words by the alphabet used and count each of the respective word's total occurence within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangle.disect_text(\n",
    "    tweets_list=valid_src_tweets.text.to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New len of list is : 19897\n",
      "First 1 elements in list are: \n",
      "\n",
      "('mahsaamini', 208211)\n"
     ]
    }
   ],
   "source": [
    "latin_hashtags = wrangle.sort_to_list(\n",
    "    dict_name='latin_hashtags',\n",
    "    num=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New len of list is : 87997\n",
      "First 1 elements in list are: \n",
      "\n",
      "('iran', 91127)\n"
     ]
    }
   ],
   "source": [
    "latin_words =wrangle.sort_to_list(\n",
    "    dict_name='latin_words',\n",
    "    num=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing each tweet into lists of words and hastags in latin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangle.tweet_to_hashtag(valid_src_tweets.text.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangle.tweet_to_text(valid_src_tweets.text.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the first 150 most occured words and hashtags in order to analyse these elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_use = []\n",
    "hashtags_to_use = []\n",
    "\n",
    "for i in range(150):\n",
    "    words_to_use.append(latin_words[i][0])\n",
    "    hashtags_to_use.append(latin_hashtags[i][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 words chosen are:\n",
      "[('iran', 91127), ('people', 64596), ('regime', 63421), ('iranian', 48115), ('islamic', 46955)]\n",
      "\n",
      "first 5 hashtags chosen are:\n",
      "[('mahsaamini', 208211), ('iranrevolution', 99783), ('iranprotests2022', 55146), ('iranprotests', 49644), ('iran', 44787)]\n"
     ]
    }
   ],
   "source": [
    "print('first 5 words chosen are:')\n",
    "print(latin_words[0:5])\n",
    "print('\\nfirst 5 hashtags chosen are:')\n",
    "print(latin_hashtags[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_np = pd.read_csv('../results/question_1/hashtags_mat150.csv').to_numpy()\n",
    "word_np = pd.read_csv('../results/question_1/words_mat150.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('republic', 'voice', 27115.0),\n",
       " ('islamic', 'freedom', 27115.0),\n",
       " ('iran', 'regime', 19205.0),\n",
       " ('iran', 'iranian', 18025.0),\n",
       " ('regime', 'people', 18025.0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_pair_occ = []\n",
    "ind_list = []\n",
    "for i in range(len(word_np)):\n",
    "    for j in range(len(word_np)):\n",
    "        val = np.max(word_np[i,j])\n",
    "        word_i = words_to_use[i]\n",
    "        word_j = words_to_use[j]\n",
    "        if i != j:\n",
    "            words_pair_occ.append((word_i, word_j, val))\n",
    "\n",
    "words_pair_occ_top5 = sorted(\n",
    "    list(set(words_pair_occ)),\n",
    "    key=lambda x: x[2],\n",
    "    reverse=True\n",
    ")[0:5]\n",
    "\n",
    "words_pair_occ_top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mahsaamini', 'iranprotests2022', 49157.0),\n",
       " ('mahsaamini', 'mahsa_amini', 34326.0),\n",
       " ('opiran', 'iranrevolution', 34326.0),\n",
       " ('mahsaamini', 'iranprotests', 28556.0),\n",
       " ('iranprotests2022', 'iranrevolution', 28556.0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags_pair_occ = []\n",
    "ind_list = []\n",
    "for i in range(len(hash_np)):\n",
    "    for j in range(len(hash_np)):\n",
    "        val = np.max(hash_np[i,j])\n",
    "        word_i = hashtags_to_use[i]\n",
    "        word_j = hashtags_to_use[j]\n",
    "        if i != j:\n",
    "            hashtags_pair_occ.append((word_i, word_j, val))\n",
    "\n",
    "hashtags_pair_occ_top5 = sorted(\n",
    "    list(set(hashtags_pair_occ)),\n",
    "    key=lambda x: x[2],\n",
    "    reverse=True\n",
    ")[0:5]\n",
    "\n",
    "hashtags_pair_occ_top5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross check each word with others and count when they occur together. Print to a matrix, save to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_mat = np.zeros((len(words_to_use), len(words_to_use)))\n",
    "for i in range(len(words_to_use) - 1):\n",
    "    for j in range(i+1, len(words_to_use)):\n",
    "        count = 0\n",
    "        for tweet in wrangle.tweet_texts_list:\n",
    "            if words_to_use[i] in tweet:\n",
    "                if words_to_use[j] in tweet:\n",
    "                    count +=1\n",
    "        words_mat[i,j] = count\n",
    "        words_mat[j,i] = count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_mat_pd = pd.DataFrame(words_mat)\n",
    "words_mat_pd.to_csv('words_mat150.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross check each hashtags with others and count when they occur together. Print to a matrix, save to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_mat = np.zeros((len(hashtags_to_use), len(hashtags_to_use)))\n",
    "for i in range(len(hashtags_to_use) - 1):\n",
    "    for j in range(i+1, len(hashtags_to_use)):\n",
    "        count = 0\n",
    "        for tweet in wrangle.tweet_hashtags_list:\n",
    "            if hashtags_to_use[i] in tweet:\n",
    "                if hashtags_to_use[j] in tweet:\n",
    "                    count +=1\n",
    "        hashtags_mat[i,j] = count\n",
    "        hashtags_mat[j,i] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_mat_pd = pd.DataFrame(hashtags_mat)\n",
    "hashtags_mat_pd.to_csv('hashtags_mat150.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have created two distinct networks with nodes being individual words or hashtags and values are the 'connection weights' based on how many times the two corresponding word or hashtag were used in an unique tweet. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Options for the visualisation of the networks, found with trial and error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTION_LARGE = \"\"\"\n",
    "    var options = {\n",
    "    \"nodes\": {\n",
    "        \"borderWidth\": 2,\n",
    "        \"opacity\": 0.8,\n",
    "        \"font\": {\n",
    "        \"size\": 20,\n",
    "        \"strokeWidth\": 10\n",
    "        }\n",
    "    },\n",
    "    \"physics\": {\n",
    "        \"forceAtlas2Based\": {\n",
    "        \"gravitationalConstant\": -20,\n",
    "        \"centralGravity\": 0.03,\n",
    "        \"springLength\": 100,\n",
    "        \"damping\": 0.9\n",
    "        },\n",
    "        \"minVelocity\": 0.75,\n",
    "        \"solver\": \"forceAtlas2Based\"\n",
    "    }\n",
    "}\"\"\"\n",
    "\n",
    "OPTION_MEDIUM = \"\"\"\n",
    "    var options = {\n",
    "    \"nodes\": {\n",
    "        \"borderWidth\": 2,\n",
    "        \"opacity\": 0.8,\n",
    "        \"font\": {\n",
    "        \"size\": 12,\n",
    "        \"strokeWidth\": 6\n",
    "        }\n",
    "    },\n",
    "    \"physics\": {\n",
    "        \"forceAtlas2Based\": {\n",
    "        \"gravitationalConstant\": -20,\n",
    "        \"centralGravity\": 0.025,\n",
    "        \"springLength\": 100,\n",
    "        \"damping\": 0.9\n",
    "        },\n",
    "        \"minVelocity\": 0.75,\n",
    "        \"solver\": \"forceAtlas2Based\"\n",
    "    }\n",
    "}\"\"\"\n",
    "\n",
    "OPTION_SMALL = \"\"\"\n",
    "    var options = {\n",
    "    \"nodes\": {\n",
    "        \"borderWidth\": 2,\n",
    "        \"opacity\": 0.8,\n",
    "        \"font\": {\n",
    "        \"size\": 10,\n",
    "        \"strokeWidth\": 5\n",
    "        }\n",
    "    },\n",
    "    \"physics\": {\n",
    "        \"forceAtlas2Based\": {\n",
    "        \"gravitationalConstant\": -20,\n",
    "        \"centralGravity\": 0.02,\n",
    "        \"springLength\": 100,\n",
    "        \"damping\": 0.9\n",
    "        },\n",
    "        \"minVelocity\": 0.75,\n",
    "        \"solver\": \"forceAtlas2Based\"\n",
    "    }\n",
    "}\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a top level function to visualise the networks created, allowing a cutoff ratio to be passed so that connections with lower magnitudes could be eliminated, so that complexity of the visualisation can be modified. Tenable cutoff ratios can be determined for each network by trial and error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "\n",
    "def show_graph(cutoff_ratio, file_name, mat_type, options):   \n",
    "    if mat_type=='hashtag':\n",
    "        to_use = hashtags_to_use\n",
    "        mat = hashtags_mat\n",
    "    elif mat_type=='word':\n",
    "        to_use = words_to_use\n",
    "        mat = words_mat\n",
    "\n",
    "    filt_words_mat = mat.copy()\n",
    "    tot = np.sum(filt_words_mat)\n",
    "    filt_words_mat[filt_words_mat/tot < cutoff_ratio] = 0\n",
    "    filt_word_mat_pd = pd.DataFrame(filt_words_mat)\n",
    "    filt_word_mat_pd.to_csv(f'{file_name}.csv')\n",
    "    net_w = Network(notebook = True, cdn_resources = 'remote')\n",
    "    for i in range(len(mat)):\n",
    "        net_w.add_node(i, label = to_use[i], value=np.sum(mat[i]))\n",
    "    for i in range(len(mat)):\n",
    "        for j in range(len(mat)):\n",
    "            if filt_words_mat[i,j] > 0:\n",
    "                net_w.add_edge(i,j, value=filt_words_mat[i,j])\n",
    "\n",
    "    net_w.set_options(options)    \n",
    "    \n",
    "    #net_w.show_buttons(filter_=['nodes', 'edges', 'physics'])\n",
    "\n",
    "    net_w.show(f'{file_name}.html')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 15 different graph representations with varying cutoff ratios and saving them as html for hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7.5e-05)\n",
      "(2, 0.00021250000000000002)\n",
      "(3, 0.00035)\n",
      "(4, 0.0004875)\n",
      "(5, 0.000625)\n",
      "(6, 0.0007625)\n",
      "(7, 0.0009)\n",
      "(8, 0.0010375)\n",
      "(9, 0.001175)\n",
      "(10, 0.0013125)\n",
      "(11, 0.0014500000000000001)\n",
      "(12, 0.0015875000000000002)\n",
      "(13, 0.001725)\n",
      "(14, 0.0018625)\n",
      "(15, 0.002)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for cutoff in np.linspace(0.000075, 0.002, num=15):\n",
    "    i += 1\n",
    "    print((i, cutoff))\n",
    "    if i < 5:\n",
    "        options = OPTION_LARGE\n",
    "    elif 5 <= i < 10:\n",
    "        options = OPTION_MEDIUM\n",
    "    elif 10 <= i:\n",
    "        options = OPTION_SMALL\n",
    "    \n",
    "    show_graph(\n",
    "        cutoff_ratio=cutoff,\n",
    "        file_name='hashtag_' + str(i),\n",
    "        mat_type='hashtag',\n",
    "        options=options\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 15 different graph representations with varying cutoff ratios and saving them as html for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.0002)\n",
      "(2, 0.0003285714285714286)\n",
      "(3, 0.00045714285714285713)\n",
      "(4, 0.0005857142857142858)\n",
      "(5, 0.0007142857142857143)\n",
      "(6, 0.0008428571428571429)\n",
      "(7, 0.0009714285714285714)\n",
      "(8, 0.0011)\n",
      "(9, 0.0012285714285714287)\n",
      "(10, 0.0013571428571428573)\n",
      "(11, 0.001485714285714286)\n",
      "(12, 0.0016142857142857144)\n",
      "(13, 0.001742857142857143)\n",
      "(14, 0.0018714285714285716)\n",
      "(15, 0.002)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for cutoff in np.linspace(0.0002, 0.002, num=15):\n",
    "    i += 1\n",
    "    print((i, cutoff))\n",
    "    if i < 5:\n",
    "        options = OPTION_LARGE\n",
    "    elif 5 <= i < 10:\n",
    "        options = OPTION_MEDIUM\n",
    "    elif 10 <= i:\n",
    "        options = OPTION_SMALL\n",
    "    \n",
    "    show_graph(\n",
    "        cutoff_ratio=cutoff,\n",
    "        file_name='word_' + str(i),\n",
    "        mat_type='word',\n",
    "        options=options\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b1f2b33e866b0bf2409397e5f58ba9cdf170d3b7f64c8f359c79998e2f88ad4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
